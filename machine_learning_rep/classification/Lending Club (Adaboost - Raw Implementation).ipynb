{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost - Raw Implementation\n",
    "\n",
    "## Lending Club Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abido\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (19,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['id', 'member_id', 'loan_amnt', 'funded_amnt', 'funded_amnt_inv',\n",
       "       'term', 'int_rate', 'installment', 'grade', 'sub_grade', 'emp_title',\n",
       "       'emp_length', 'home_ownership', 'annual_inc', 'is_inc_v', 'issue_d',\n",
       "       'loan_status', 'pymnt_plan', 'url', 'desc', 'purpose', 'title',\n",
       "       'zip_code', 'addr_state', 'dti', 'delinq_2yrs', 'earliest_cr_line',\n",
       "       'inq_last_6mths', 'mths_since_last_delinq', 'mths_since_last_record',\n",
       "       'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc',\n",
       "       'initial_list_status', 'out_prncp', 'out_prncp_inv', 'total_pymnt',\n",
       "       'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',\n",
       "       'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
       "       'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d',\n",
       "       'collections_12_mths_ex_med', 'mths_since_last_major_derog',\n",
       "       'policy_code', 'not_compliant', 'status', 'inactive_loans', 'bad_loans',\n",
       "       'emp_length_num', 'grade_num', 'sub_grade_num', 'delinq_2yrs_zero',\n",
       "       'pub_rec_zero', 'collections_12_mths_zero', 'short_emp',\n",
       "       'payment_inc_ratio', 'final_d', 'last_delinq_none', 'last_record_none',\n",
       "       'last_major_derog_none'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "loans = pd.read_csv(\"lending-club-data.csv\")\n",
    "\n",
    "\n",
    "loans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122607, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "0           1        0        1        0        0        0        0        0   \n",
       "1          -1        0        0        1        0        0        0        0   \n",
       "2           1        0        0        1        0        0        0        0   \n",
       "3           1        0        0        1        0        0        0        0   \n",
       "4           1        1        0        0        0        0        0        0   \n",
       "\n",
       "   term_ 36 months  term_ 60 months         ...           \\\n",
       "0                1                0         ...            \n",
       "1                0                1         ...            \n",
       "2                1                0         ...            \n",
       "3                1                0         ...            \n",
       "4                1                0         ...            \n",
       "\n",
       "   emp_length_10+ years  emp_length_2 years  emp_length_3 years  \\\n",
       "0                     1                   0                   0   \n",
       "1                     0                   0                   0   \n",
       "2                     1                   0                   0   \n",
       "3                     1                   0                   0   \n",
       "4                     0                   0                   1   \n",
       "\n",
       "   emp_length_4 years  emp_length_5 years  emp_length_6 years  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   emp_length_7 years  emp_length_8 years  emp_length_9 years  \\\n",
       "0                   0                   0                   0   \n",
       "1                   0                   0                   0   \n",
       "2                   0                   0                   0   \n",
       "3                   0                   0                   0   \n",
       "4                   0                   0                   0   \n",
       "\n",
       "   emp_length_< 1 year  \n",
       "0                    0  \n",
       "1                    1  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "\n",
    "# safe_loans =  1 => safe\n",
    "# safe_loans = -1 => risky\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.drop('bad_loans', axis = 1)\n",
    "\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]\n",
    "loans = pd.get_dummies(loans)\n",
    "print(loans.shape)\n",
    "loans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37224, 25)\n",
      "(9284, 25)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safe_loans</th>\n",
       "      <th>grade_A</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>term_ 36 months</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>...</th>\n",
       "      <th>emp_length_10+ years</th>\n",
       "      <th>emp_length_2 years</th>\n",
       "      <th>emp_length_3 years</th>\n",
       "      <th>emp_length_4 years</th>\n",
       "      <th>emp_length_5 years</th>\n",
       "      <th>emp_length_6 years</th>\n",
       "      <th>emp_length_7 years</th>\n",
       "      <th>emp_length_8 years</th>\n",
       "      <th>emp_length_9 years</th>\n",
       "      <th>emp_length_&lt; 1 year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    safe_loans  grade_A  grade_B  grade_C  grade_D  grade_E  grade_F  grade_G  \\\n",
       "1           -1        0        0        1        0        0        0        0   \n",
       "6           -1        0        0        0        0        0        1        0   \n",
       "7           -1        0        1        0        0        0        0        0   \n",
       "10          -1        0        0        1        0        0        0        0   \n",
       "12          -1        0        1        0        0        0        0        0   \n",
       "\n",
       "    term_ 36 months  term_ 60 months         ...           \\\n",
       "1                 0                1         ...            \n",
       "6                 0                1         ...            \n",
       "7                 0                1         ...            \n",
       "10                1                0         ...            \n",
       "12                1                0         ...            \n",
       "\n",
       "    emp_length_10+ years  emp_length_2 years  emp_length_3 years  \\\n",
       "1                      0                   0                   0   \n",
       "6                      0                   0                   0   \n",
       "7                      0                   0                   0   \n",
       "10                     0                   0                   0   \n",
       "12                     0                   0                   1   \n",
       "\n",
       "    emp_length_4 years  emp_length_5 years  emp_length_6 years  \\\n",
       "1                    0                   0                   0   \n",
       "6                    1                   0                   0   \n",
       "7                    0                   0                   0   \n",
       "10                   0                   0                   0   \n",
       "12                   0                   0                   0   \n",
       "\n",
       "    emp_length_7 years  emp_length_8 years  emp_length_9 years  \\\n",
       "1                    0                   0                   0   \n",
       "6                    0                   0                   0   \n",
       "7                    0                   0                   0   \n",
       "10                   0                   0                   0   \n",
       "12                   0                   0                   0   \n",
       "\n",
       "    emp_length_< 1 year  \n",
       "1                     1  \n",
       "6                     0  \n",
       "7                     1  \n",
       "10                    1  \n",
       "12                    0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"module-8-assignment-2-train-idx.json\") as json_file:\n",
    "    train_idx = json.load(json_file)\n",
    "    \n",
    "with open(\"module-8-assignment-2-test-idx.json\") as json_file:\n",
    "    test_idx = json.load(json_file)\n",
    "\n",
    "train_data = loans.iloc[train_idx]\n",
    "test_data = loans.iloc[test_idx]\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_positive <= weighted_mistakes_all_negative:\n",
    "        return (weighted_mistakes_all_positive, 1)\n",
    "    else:\n",
    "        return (weighted_mistakes_all_negative, -1)\n",
    "    \n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "                    \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes ) / sum(data_weights)\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    leaf['prediction'] = best_class ## YOUR CODE HERE\n",
    "    \n",
    "    return leaf\n",
    "\n",
    "\n",
    "\n",
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print(\"Stopping condition 1 reached.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached.\")\n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print(\"Reached maximum depth. Stopping for now.\")\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split)))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print(\"Creating leaf node.\")\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])   \n",
    "\n",
    "\n",
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)\n",
    "\n",
    "\n",
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x), axis=1)\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "Split on feature grade_F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "Split on feature grade_D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = np.array([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, list(train_data.columns[1:]), target,\n",
    "                         example_data_weights, max_depth=2)\n",
    "\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = np.array([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in range(num_tree_stumps):\n",
    "        print('=====================================================')\n",
    "        print('Adaboost Iteration %d' % t)\n",
    "        print('=====================================================')\n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weighted_error = sum(alpha[is_wrong])/sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight = 0.5*log((1-weighted_error)/weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment\n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha = alpha*adjustment\n",
    "        alpha = alpha/sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.15802933659263743,\n",
       "  0.1768236329364191,\n",
       "  0.09311888971129693,\n",
       "  0.07288885525840554,\n",
       "  0.06706306914118143,\n",
       "  0.06456916961644447,\n",
       "  0.05456055779178564,\n",
       "  0.04351093673362621,\n",
       "  0.02898871150041245,\n",
       "  0.01933343817072769],\n",
       " [{'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'term_ 36 months',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_A',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_D',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'home_ownership_MORTGAGE',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_B',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_E',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_A',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_F',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_A',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1}},\n",
       "  {'is_leaf': False,\n",
       "   'prediction': None,\n",
       "   'splitting_feature': 'grade_F',\n",
       "   'left': {'splitting_feature': None, 'is_leaf': True, 'prediction': 1},\n",
       "   'right': {'splitting_feature': None, 'is_leaf': True, 'prediction': -1}}])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost_with_tree_stumps(train_data,list(train_data.columns[1:]),target,num_tree_stumps = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaboost Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = np.array([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x), axis=1)\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        scores = scores + stump_weights[i] * predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_10+ years. (26901, 10323)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26901 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10323 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_G. (36788, 436)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (36788 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (436 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_3 years. (34099, 3125)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34099 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3125 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature home_ownership_OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_G. (36788, 436)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (36788 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (436 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature emp_length_4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_G. (36788, 436)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (36788 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (436 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature term_ 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "Split on feature grade_C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, list(train_data.columns[1:]), target, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.4216365785514722\n",
      "Iteration 2, training error = 0.4334300451321728\n",
      "Iteration 3, training error = 0.40003761014399314\n",
      "Iteration 4, training error = 0.40003761014399314\n",
      "Iteration 5, training error = 0.3847249086610789\n",
      "Iteration 6, training error = 0.38461745110681284\n",
      "Iteration 7, training error = 0.3827638082957232\n",
      "Iteration 8, training error = 0.38461745110681284\n",
      "Iteration 9, training error = 0.3827638082957232\n",
      "Iteration 10, training error = 0.3814474532559639\n",
      "Iteration 11, training error = 0.3814474532559639\n",
      "Iteration 12, training error = 0.3814474532559639\n",
      "Iteration 13, training error = 0.3814474532559639\n",
      "Iteration 14, training error = 0.3814474532559639\n",
      "Iteration 15, training error = 0.3814474532559639\n",
      "Iteration 16, training error = 0.3814205888673974\n",
      "Iteration 17, training error = 0.3814205888673974\n",
      "Iteration 18, training error = 0.3816355039759295\n",
      "Iteration 19, training error = 0.3816355039759295\n",
      "Iteration 20, training error = 0.3816355039759295\n",
      "Iteration 21, training error = 0.3814205888673974\n",
      "Iteration 22, training error = 0.382441435632925\n",
      "Iteration 23, training error = 0.3814205888673974\n",
      "Iteration 24, training error = 0.38241457124435846\n",
      "Iteration 25, training error = 0.3814205888673974\n",
      "Iteration 26, training error = 0.38241457124435846\n",
      "Iteration 27, training error = 0.3813668600902643\n",
      "Iteration 28, training error = 0.38155491081022996\n",
      "Iteration 29, training error = 0.38187728347302813\n",
      "Iteration 30, training error = 0.38182355469589513\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in range(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = sum(predictions!=train_data[target])/float(len(train_data))\n",
    "    error_all.append(error)\n",
    "    print(\"Iteration %s, training error = %s\" % (n, error_all[n-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2241a6fac50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "# plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### USE LATER IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "#visualize the training error\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=2.0, label='Training error')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.show()\n",
    "\n",
    "#validation error\n",
    "validation_error_all = []\n",
    "for n in range(30):\n",
    "\tpredictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], validation_data)\n",
    "\terror = sum(predictions!=validation_data[target])/float(len(validation_data))\n",
    "\tvalidation_error_all.append(error)\n",
    "\tprint('Iteration %s, validation error %s' % (n+1, validation_error_all[n]))\n",
    "\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=2.0, label='Training error')\n",
    "plt.plot(range(1,31), validation_error_all, '-', linewidth=2.0, label='Validation error')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
