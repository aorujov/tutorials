{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cat_or_Dogs_small (Augmentation + Dropout).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "lRCt_uyfHeHz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Notebook Info:\n",
        "\n",
        "* Dataset - Cat vs Dogs (3k images)\n",
        "* Network - CNN with dropout\n",
        "* Additional - ImageDataGenerator(Augmentation)"
      ]
    },
    {
      "metadata": {
        "id": "JBYisRd_F-Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5396
        },
        "outputId": "8b3c8e28-f0fe-4dbd-917d-64aa533e9008"
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-25 15:27:10--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 2607:f8b0:400e:c02::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  65.9MB/s    in 1.0s    \n",
            "\n",
            "2019-04-25 15:27:11 (65.9 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 0.6684 - acc: 0.6070\n",
            " - 12s - loss: 0.6880 - acc: 0.5260 - val_loss: 0.6684 - val_acc: 0.6070\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.6301 - acc: 0.6370\n",
            " - 9s - loss: 0.6519 - acc: 0.6125 - val_loss: 0.6301 - val_acc: 0.6370\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.6022 - acc: 0.6650\n",
            " - 8s - loss: 0.5997 - acc: 0.6860 - val_loss: 0.6022 - val_acc: 0.6650\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.6111 - acc: 0.6600\n",
            " - 8s - loss: 0.5642 - acc: 0.7070 - val_loss: 0.6111 - val_acc: 0.6600\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.5899 - acc: 0.6810\n",
            " - 8s - loss: 0.5344 - acc: 0.7255 - val_loss: 0.5899 - val_acc: 0.6810\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.5545 - acc: 0.7170\n",
            " - 8s - loss: 0.5099 - acc: 0.7505 - val_loss: 0.5545 - val_acc: 0.7170\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.5399 - acc: 0.7210\n",
            " - 8s - loss: 0.4772 - acc: 0.7625 - val_loss: 0.5399 - val_acc: 0.7210\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.5477 - acc: 0.7250\n",
            " - 9s - loss: 0.4497 - acc: 0.7760 - val_loss: 0.5477 - val_acc: 0.7250\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.5635 - acc: 0.7210\n",
            " - 9s - loss: 0.4257 - acc: 0.8040 - val_loss: 0.5635 - val_acc: 0.7210\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.5460 - acc: 0.7240\n",
            " - 8s - loss: 0.4025 - acc: 0.8175 - val_loss: 0.5460 - val_acc: 0.7240\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.5065 - acc: 0.7580\n",
            " - 8s - loss: 0.3791 - acc: 0.8335 - val_loss: 0.5065 - val_acc: 0.7580\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.5887 - acc: 0.7230\n",
            " - 8s - loss: 0.3582 - acc: 0.8430 - val_loss: 0.5887 - val_acc: 0.7230\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6296 - acc: 0.7100\n",
            " - 8s - loss: 0.3329 - acc: 0.8500 - val_loss: 0.6296 - val_acc: 0.7100\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 0.5098 - acc: 0.7660\n",
            " - 8s - loss: 0.3153 - acc: 0.8655 - val_loss: 0.5098 - val_acc: 0.7660\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.5178 - acc: 0.7560\n",
            " - 8s - loss: 0.2927 - acc: 0.8795 - val_loss: 0.5178 - val_acc: 0.7560\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.5331 - acc: 0.7710\n",
            " - 9s - loss: 0.2720 - acc: 0.8880 - val_loss: 0.5331 - val_acc: 0.7710\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5467 - acc: 0.7610\n",
            " - 9s - loss: 0.2441 - acc: 0.9010 - val_loss: 0.5467 - val_acc: 0.7610\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 64ms/step - loss: 0.5605 - acc: 0.7690\n",
            " - 9s - loss: 0.2363 - acc: 0.9095 - val_loss: 0.5605 - val_acc: 0.7690\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6201 - acc: 0.7570\n",
            " - 8s - loss: 0.2150 - acc: 0.9185 - val_loss: 0.6201 - val_acc: 0.7570\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6185 - acc: 0.7380\n",
            " - 8s - loss: 0.1987 - acc: 0.9325 - val_loss: 0.6185 - val_acc: 0.7380\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6088 - acc: 0.7470\n",
            " - 8s - loss: 0.1732 - acc: 0.9400 - val_loss: 0.6088 - val_acc: 0.7470\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6051 - acc: 0.7760\n",
            " - 8s - loss: 0.1602 - acc: 0.9420 - val_loss: 0.6051 - val_acc: 0.7760\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6218 - acc: 0.7800\n",
            " - 8s - loss: 0.1427 - acc: 0.9530 - val_loss: 0.6218 - val_acc: 0.7800\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6817 - acc: 0.7300\n",
            " - 8s - loss: 0.1277 - acc: 0.9540 - val_loss: 0.6817 - val_acc: 0.7300\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.6635 - acc: 0.7450\n",
            " - 8s - loss: 0.1123 - acc: 0.9625 - val_loss: 0.6635 - val_acc: 0.7450\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.7266 - acc: 0.7570\n",
            " - 8s - loss: 0.1006 - acc: 0.9645 - val_loss: 0.7266 - val_acc: 0.7570\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 0.6990 - acc: 0.7740\n",
            " - 9s - loss: 0.0883 - acc: 0.9720 - val_loss: 0.6990 - val_acc: 0.7740\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.7526 - acc: 0.7580\n",
            " - 9s - loss: 0.0792 - acc: 0.9755 - val_loss: 0.7526 - val_acc: 0.7580\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.7512 - acc: 0.7590\n",
            " - 8s - loss: 0.0617 - acc: 0.9810 - val_loss: 0.7512 - val_acc: 0.7590\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.7639 - acc: 0.7610\n",
            " - 8s - loss: 0.0607 - acc: 0.9825 - val_loss: 0.7639 - val_acc: 0.7610\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 0.7839 - acc: 0.7640\n",
            " - 8s - loss: 0.0493 - acc: 0.9870 - val_loss: 0.7839 - val_acc: 0.7640\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.8274 - acc: 0.7530\n",
            " - 8s - loss: 0.0479 - acc: 0.9865 - val_loss: 0.8274 - val_acc: 0.7530\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.9027 - acc: 0.7600\n",
            " - 8s - loss: 0.0369 - acc: 0.9900 - val_loss: 0.9027 - val_acc: 0.7600\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.9804 - acc: 0.7380\n",
            " - 8s - loss: 0.0314 - acc: 0.9920 - val_loss: 0.9804 - val_acc: 0.7380\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 0.9690 - acc: 0.7660\n",
            " - 8s - loss: 0.0284 - acc: 0.9925 - val_loss: 0.9690 - val_acc: 0.7660\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 0.9682 - acc: 0.7500\n",
            " - 8s - loss: 0.0302 - acc: 0.9900 - val_loss: 0.9682 - val_acc: 0.7500\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 4s 71ms/step - loss: 1.0119 - acc: 0.7690\n",
            " - 9s - loss: 0.0242 - acc: 0.9945 - val_loss: 1.0119 - val_acc: 0.7690\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.9686 - acc: 0.7630\n",
            " - 9s - loss: 0.0247 - acc: 0.9935 - val_loss: 0.9686 - val_acc: 0.7630\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.1002 - acc: 0.7540\n",
            " - 9s - loss: 0.0179 - acc: 0.9960 - val_loss: 1.1002 - val_acc: 0.7540\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.0630 - acc: 0.7480\n",
            " - 8s - loss: 0.0176 - acc: 0.9955 - val_loss: 1.0630 - val_acc: 0.7480\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.3544 - acc: 0.7460\n",
            " - 8s - loss: 0.0137 - acc: 0.9975 - val_loss: 1.3544 - val_acc: 0.7460\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.1262 - acc: 0.7550\n",
            " - 8s - loss: 0.0172 - acc: 0.9940 - val_loss: 1.1262 - val_acc: 0.7550\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.1578 - acc: 0.7450\n",
            " - 8s - loss: 0.0129 - acc: 0.9970 - val_loss: 1.1578 - val_acc: 0.7450\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.1985 - acc: 0.7630\n",
            " - 8s - loss: 0.0121 - acc: 0.9975 - val_loss: 1.1985 - val_acc: 0.7630\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.1535 - acc: 0.7600\n",
            " - 8s - loss: 0.0138 - acc: 0.9955 - val_loss: 1.1535 - val_acc: 0.7600\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 1.1601 - acc: 0.7550\n",
            " - 8s - loss: 0.0133 - acc: 0.9950 - val_loss: 1.1601 - val_acc: 0.7550\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.2076 - acc: 0.7670\n",
            " - 9s - loss: 0.0112 - acc: 0.9965 - val_loss: 1.2076 - val_acc: 0.7670\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.2746 - acc: 0.7590\n",
            " - 8s - loss: 0.0123 - acc: 0.9960 - val_loss: 1.2746 - val_acc: 0.7590\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.3001 - acc: 0.7500\n",
            " - 8s - loss: 0.0118 - acc: 0.9975 - val_loss: 1.3001 - val_acc: 0.7500\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.3357 - acc: 0.7560\n",
            " - 8s - loss: 0.0065 - acc: 0.9960 - val_loss: 1.3357 - val_acc: 0.7560\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.2957 - acc: 0.7570\n",
            " - 8s - loss: 0.0187 - acc: 0.9930 - val_loss: 1.2957 - val_acc: 0.7570\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4088 - acc: 0.7560\n",
            " - 8s - loss: 0.0033 - acc: 0.9990 - val_loss: 1.4088 - val_acc: 0.7560\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 1.3148 - acc: 0.7450\n",
            " - 9s - loss: 0.0085 - acc: 0.9980 - val_loss: 1.3148 - val_acc: 0.7450\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 1.4675 - acc: 0.7550\n",
            " - 9s - loss: 0.0065 - acc: 0.9980 - val_loss: 1.4675 - val_acc: 0.7550\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 62ms/step - loss: 1.7324 - acc: 0.7240\n",
            " - 8s - loss: 0.0088 - acc: 0.9970 - val_loss: 1.7324 - val_acc: 0.7240\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 1.3733 - acc: 0.7550\n",
            " - 9s - loss: 0.0081 - acc: 0.9965 - val_loss: 1.3733 - val_acc: 0.7550\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.3787 - acc: 0.7640\n",
            " - 9s - loss: 0.0029 - acc: 0.9990 - val_loss: 1.3787 - val_acc: 0.7640\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4416 - acc: 0.7660\n",
            " - 8s - loss: 0.0065 - acc: 0.9985 - val_loss: 1.4416 - val_acc: 0.7660\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4696 - acc: 0.7580\n",
            " - 8s - loss: 0.0071 - acc: 0.9975 - val_loss: 1.4696 - val_acc: 0.7580\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4555 - acc: 0.7660\n",
            " - 8s - loss: 0.0059 - acc: 0.9980 - val_loss: 1.4555 - val_acc: 0.7660\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 2.6724 - acc: 0.6620\n",
            " - 8s - loss: 0.0048 - acc: 0.9990 - val_loss: 2.6724 - val_acc: 0.6620\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4697 - acc: 0.7530\n",
            " - 8s - loss: 0.0096 - acc: 0.9970 - val_loss: 1.4697 - val_acc: 0.7530\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.4792 - acc: 0.7550\n",
            " - 8s - loss: 0.0048 - acc: 0.9990 - val_loss: 1.4792 - val_acc: 0.7550\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.5632 - acc: 0.7560\n",
            " - 8s - loss: 0.0055 - acc: 0.9975 - val_loss: 1.5632 - val_acc: 0.7560\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 63ms/step - loss: 1.4868 - acc: 0.7640\n",
            " - 8s - loss: 0.0049 - acc: 0.9980 - val_loss: 1.4868 - val_acc: 0.7640\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 1.4733 - acc: 0.7620\n",
            " - 9s - loss: 0.0044 - acc: 0.9980 - val_loss: 1.4733 - val_acc: 0.7620\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.5570 - acc: 0.7560\n",
            " - 8s - loss: 0.0045 - acc: 0.9990 - val_loss: 1.5570 - val_acc: 0.7560\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6131 - acc: 0.7580\n",
            " - 8s - loss: 0.0042 - acc: 0.9985 - val_loss: 1.6131 - val_acc: 0.7580\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.5825 - acc: 0.7560\n",
            " - 8s - loss: 0.0080 - acc: 0.9985 - val_loss: 1.5825 - val_acc: 0.7560\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6698 - acc: 0.7550\n",
            " - 8s - loss: 0.0035 - acc: 0.9985 - val_loss: 1.6698 - val_acc: 0.7550\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.7101 - acc: 0.7460\n",
            " - 8s - loss: 0.0063 - acc: 0.9990 - val_loss: 1.7101 - val_acc: 0.7460\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6681 - acc: 0.7580\n",
            " - 8s - loss: 0.0095 - acc: 0.9985 - val_loss: 1.6681 - val_acc: 0.7580\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6335 - acc: 0.7540\n",
            " - 8s - loss: 0.0021 - acc: 0.9990 - val_loss: 1.6335 - val_acc: 0.7540\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.8965 - acc: 0.7450\n",
            " - 8s - loss: 0.0064 - acc: 0.9980 - val_loss: 1.8965 - val_acc: 0.7450\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 1.5998 - acc: 0.7520\n",
            " - 9s - loss: 0.0049 - acc: 0.9975 - val_loss: 1.5998 - val_acc: 0.7520\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 65ms/step - loss: 1.6475 - acc: 0.7660\n",
            " - 9s - loss: 0.0085 - acc: 0.9975 - val_loss: 1.6475 - val_acc: 0.7660\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.5917 - acc: 0.7610\n",
            " - 8s - loss: 0.0059 - acc: 0.9975 - val_loss: 1.5917 - val_acc: 0.7610\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.5970 - acc: 0.7640\n",
            " - 8s - loss: 0.0040 - acc: 0.9985 - val_loss: 1.5970 - val_acc: 0.7640\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.7213 - acc: 0.7440\n",
            " - 8s - loss: 0.0032 - acc: 0.9990 - val_loss: 1.7213 - val_acc: 0.7440\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.7479 - acc: 0.7640\n",
            " - 8s - loss: 0.0048 - acc: 0.9985 - val_loss: 1.7479 - val_acc: 0.7640\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.5953 - acc: 0.7580\n",
            " - 8s - loss: 0.0118 - acc: 0.9975 - val_loss: 1.5953 - val_acc: 0.7580\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6325 - acc: 0.7570\n",
            " - 8s - loss: 6.5054e-04 - acc: 0.9995 - val_loss: 1.6325 - val_acc: 0.7570\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6590 - acc: 0.7680\n",
            " - 8s - loss: 0.0016 - acc: 0.9995 - val_loss: 1.6590 - val_acc: 0.7680\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 2.5478 - acc: 0.6950\n",
            " - 8s - loss: 0.0049 - acc: 0.9980 - val_loss: 2.5478 - val_acc: 0.6950\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 1.6817 - acc: 0.7720\n",
            " - 9s - loss: 0.0035 - acc: 0.9995 - val_loss: 1.6817 - val_acc: 0.7720\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.6399 - acc: 0.7660\n",
            " - 9s - loss: 0.0058 - acc: 0.9980 - val_loss: 1.6399 - val_acc: 0.7660\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6803 - acc: 0.7660\n",
            " - 8s - loss: 0.0036 - acc: 0.9980 - val_loss: 1.6803 - val_acc: 0.7660\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6826 - acc: 0.7620\n",
            " - 8s - loss: 0.0074 - acc: 0.9985 - val_loss: 1.6826 - val_acc: 0.7620\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6365 - acc: 0.7760\n",
            " - 8s - loss: 0.0105 - acc: 0.9975 - val_loss: 1.6365 - val_acc: 0.7760\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.7086 - acc: 0.7600\n",
            " - 8s - loss: 0.0145 - acc: 0.9970 - val_loss: 1.7086 - val_acc: 0.7600\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 4s 73ms/step - loss: 1.7075 - acc: 0.7650\n",
            " - 10s - loss: 4.4559e-04 - acc: 1.0000 - val_loss: 1.7075 - val_acc: 0.7650\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.6866 - acc: 0.7620\n",
            " - 8s - loss: 0.0070 - acc: 0.9980 - val_loss: 1.6866 - val_acc: 0.7620\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.7845 - acc: 0.7610\n",
            " - 8s - loss: 3.5253e-05 - acc: 1.0000 - val_loss: 1.7845 - val_acc: 0.7610\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.7862 - acc: 0.7540\n",
            " - 8s - loss: 0.0173 - acc: 0.9965 - val_loss: 1.7862 - val_acc: 0.7540\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 4s 71ms/step - loss: 1.8900 - acc: 0.7550\n",
            " - 9s - loss: 0.0043 - acc: 0.9985 - val_loss: 1.8900 - val_acc: 0.7550\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.9134 - acc: 0.7530\n",
            " - 8s - loss: 0.0181 - acc: 0.9965 - val_loss: 1.9134 - val_acc: 0.7530\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.8117 - acc: 0.7670\n",
            " - 8s - loss: 2.2871e-04 - acc: 1.0000 - val_loss: 1.8117 - val_acc: 0.7670\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.9499 - acc: 0.7510\n",
            " - 8s - loss: 0.0037 - acc: 0.9985 - val_loss: 1.9499 - val_acc: 0.7510\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 3s 61ms/step - loss: 1.9018 - acc: 0.7560\n",
            " - 8s - loss: 0.0069 - acc: 0.9990 - val_loss: 1.9018 - val_acc: 0.7560\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 60ms/step - loss: 1.8919 - acc: 0.7650\n",
            " - 8s - loss: 2.1953e-04 - acc: 1.0000 - val_loss: 1.8919 - val_acc: 0.7650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G_eLM4JXGADd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "098aa30f-06bf-4173-9e1e-416271575ad8"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXBby4O6cc1no/wOqKqe0F2Tr3AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "rKgvVu8xIoQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Analysis of training:\n",
        "\n",
        "The Training Accuracy is close to 100%, and the validation accuracy is in the 70%-80% range. This is a great example of overfitting -- which in short means that it can do very well with images it has seen before, but not so well with images it hasn't. Let's see if we can do better to avoid overfitting -- and one simple method is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too.\n",
        "\n",
        "What if we tweak with the images to change this up a bit -- rotate the image, squash it, etc. That's what image augementation is all about. And there's an API that makes it easy...\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Okn0D8wPKUVh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Training with Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "wxIpXCGtGAF-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5275
        },
        "outputId": "77daa42a-2240-457e-aa0f-84b91527791e"
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,  \n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-25 15:42:49--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 2607:f8b0:400e:c03::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   121MB/s    in 0.5s    \n",
            "\n",
            "2019-04-25 15:42:50 (121 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "Epoch 1/100\n",
            "50/50 [==============================] - 4s 85ms/step - loss: 0.6813 - acc: 0.5680\n",
            " - 17s - loss: 0.6920 - acc: 0.5290 - val_loss: 0.6813 - val_acc: 0.5680\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.6662 - acc: 0.5820\n",
            " - 16s - loss: 0.6730 - acc: 0.5790 - val_loss: 0.6662 - val_acc: 0.5820\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.6358 - acc: 0.6210\n",
            " - 15s - loss: 0.6584 - acc: 0.6040 - val_loss: 0.6358 - val_acc: 0.6210\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.7163 - acc: 0.5420\n",
            " - 15s - loss: 0.6374 - acc: 0.6395 - val_loss: 0.7163 - val_acc: 0.5420\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5966 - acc: 0.6980\n",
            " - 15s - loss: 0.6310 - acc: 0.6415 - val_loss: 0.5966 - val_acc: 0.6980\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 4s 78ms/step - loss: 0.5805 - acc: 0.6890\n",
            " - 16s - loss: 0.6124 - acc: 0.6495 - val_loss: 0.5805 - val_acc: 0.6890\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5690 - acc: 0.6990\n",
            " - 16s - loss: 0.5994 - acc: 0.6735 - val_loss: 0.5690 - val_acc: 0.6990\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 4s 71ms/step - loss: 0.5786 - acc: 0.6880\n",
            " - 15s - loss: 0.5952 - acc: 0.6825 - val_loss: 0.5786 - val_acc: 0.6880\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5877 - acc: 0.6670\n",
            " - 17s - loss: 0.5860 - acc: 0.6930 - val_loss: 0.5877 - val_acc: 0.6670\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5484 - acc: 0.7190\n",
            " - 15s - loss: 0.5810 - acc: 0.6930 - val_loss: 0.5484 - val_acc: 0.7190\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 4s 75ms/step - loss: 0.5578 - acc: 0.7040\n",
            " - 15s - loss: 0.5798 - acc: 0.6990 - val_loss: 0.5578 - val_acc: 0.7040\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5358 - acc: 0.7230\n",
            " - 17s - loss: 0.5692 - acc: 0.7010 - val_loss: 0.5358 - val_acc: 0.7230\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5279 - acc: 0.7360\n",
            " - 15s - loss: 0.5644 - acc: 0.7015 - val_loss: 0.5279 - val_acc: 0.7360\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5489 - acc: 0.6960\n",
            " - 15s - loss: 0.5564 - acc: 0.7035 - val_loss: 0.5489 - val_acc: 0.6960\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5278 - acc: 0.7230\n",
            " - 15s - loss: 0.5486 - acc: 0.7155 - val_loss: 0.5278 - val_acc: 0.7230\n",
            "Epoch 16/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5557 - acc: 0.7100\n",
            " - 15s - loss: 0.5461 - acc: 0.7230 - val_loss: 0.5557 - val_acc: 0.7100\n",
            "Epoch 17/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5319 - acc: 0.7080\n",
            " - 17s - loss: 0.5542 - acc: 0.7075 - val_loss: 0.5319 - val_acc: 0.7080\n",
            "Epoch 18/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5128 - acc: 0.7300\n",
            " - 15s - loss: 0.5484 - acc: 0.7160 - val_loss: 0.5128 - val_acc: 0.7300\n",
            "Epoch 19/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4947 - acc: 0.7540\n",
            " - 15s - loss: 0.5412 - acc: 0.7230 - val_loss: 0.4947 - val_acc: 0.7540\n",
            "Epoch 20/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5085 - acc: 0.7420\n",
            " - 16s - loss: 0.5246 - acc: 0.7360 - val_loss: 0.5085 - val_acc: 0.7420\n",
            "Epoch 21/100\n",
            "50/50 [==============================] - 4s 71ms/step - loss: 0.4873 - acc: 0.7530\n",
            " - 15s - loss: 0.5283 - acc: 0.7305 - val_loss: 0.4873 - val_acc: 0.7530\n",
            "Epoch 22/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4904 - acc: 0.7460\n",
            " - 17s - loss: 0.5205 - acc: 0.7420 - val_loss: 0.4904 - val_acc: 0.7460\n",
            "Epoch 23/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.5053 - acc: 0.7300\n",
            " - 15s - loss: 0.5120 - acc: 0.7320 - val_loss: 0.5053 - val_acc: 0.7300\n",
            "Epoch 24/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4879 - acc: 0.7460\n",
            " - 15s - loss: 0.5053 - acc: 0.7485 - val_loss: 0.4879 - val_acc: 0.7460\n",
            "Epoch 25/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4761 - acc: 0.7500\n",
            " - 15s - loss: 0.5126 - acc: 0.7495 - val_loss: 0.4761 - val_acc: 0.7500\n",
            "Epoch 26/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4738 - acc: 0.7600\n",
            " - 15s - loss: 0.5133 - acc: 0.7445 - val_loss: 0.4738 - val_acc: 0.7600\n",
            "Epoch 27/100\n",
            "50/50 [==============================] - 4s 77ms/step - loss: 0.4898 - acc: 0.7570\n",
            " - 17s - loss: 0.5133 - acc: 0.7500 - val_loss: 0.4898 - val_acc: 0.7570\n",
            "Epoch 28/100\n",
            "50/50 [==============================] - 4s 80ms/step - loss: 0.4986 - acc: 0.7400\n",
            " - 16s - loss: 0.5220 - acc: 0.7435 - val_loss: 0.4986 - val_acc: 0.7400\n",
            "Epoch 29/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4928 - acc: 0.7540\n",
            " - 17s - loss: 0.4992 - acc: 0.7470 - val_loss: 0.4928 - val_acc: 0.7540\n",
            "Epoch 30/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4779 - acc: 0.7670\n",
            " - 15s - loss: 0.5006 - acc: 0.7535 - val_loss: 0.4779 - val_acc: 0.7670\n",
            "Epoch 31/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4745 - acc: 0.7660\n",
            " - 15s - loss: 0.4982 - acc: 0.7580 - val_loss: 0.4745 - val_acc: 0.7660\n",
            "Epoch 32/100\n",
            "50/50 [==============================] - 4s 81ms/step - loss: 0.4635 - acc: 0.7750\n",
            " - 17s - loss: 0.4873 - acc: 0.7600 - val_loss: 0.4635 - val_acc: 0.7750\n",
            "Epoch 33/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4929 - acc: 0.7490\n",
            " - 15s - loss: 0.4866 - acc: 0.7690 - val_loss: 0.4929 - val_acc: 0.7490\n",
            "Epoch 34/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4804 - acc: 0.7660\n",
            " - 15s - loss: 0.4844 - acc: 0.7660 - val_loss: 0.4804 - val_acc: 0.7660\n",
            "Epoch 35/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4988 - acc: 0.7330\n",
            " - 15s - loss: 0.4747 - acc: 0.7825 - val_loss: 0.4988 - val_acc: 0.7330\n",
            "Epoch 36/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4533 - acc: 0.7980\n",
            " - 15s - loss: 0.4828 - acc: 0.7680 - val_loss: 0.4533 - val_acc: 0.7980\n",
            "Epoch 37/100\n",
            "50/50 [==============================] - 4s 84ms/step - loss: 0.4630 - acc: 0.7760\n",
            " - 16s - loss: 0.4673 - acc: 0.7780 - val_loss: 0.4630 - val_acc: 0.7760\n",
            "Epoch 38/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4716 - acc: 0.7750\n",
            " - 16s - loss: 0.4756 - acc: 0.7670 - val_loss: 0.4716 - val_acc: 0.7750\n",
            "Epoch 39/100\n",
            "50/50 [==============================] - 4s 72ms/step - loss: 0.4874 - acc: 0.7810\n",
            " - 16s - loss: 0.4666 - acc: 0.7780 - val_loss: 0.4874 - val_acc: 0.7810\n",
            "Epoch 40/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4551 - acc: 0.7730\n",
            " - 15s - loss: 0.4682 - acc: 0.7760 - val_loss: 0.4551 - val_acc: 0.7730\n",
            "Epoch 41/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4729 - acc: 0.7890\n",
            " - 15s - loss: 0.4631 - acc: 0.7830 - val_loss: 0.4729 - val_acc: 0.7890\n",
            "Epoch 42/100\n",
            "50/50 [==============================] - 4s 84ms/step - loss: 0.4876 - acc: 0.7620\n",
            " - 16s - loss: 0.4585 - acc: 0.7795 - val_loss: 0.4876 - val_acc: 0.7620\n",
            "Epoch 43/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4582 - acc: 0.7870\n",
            " - 16s - loss: 0.4662 - acc: 0.7765 - val_loss: 0.4582 - val_acc: 0.7870\n",
            "Epoch 44/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4665 - acc: 0.7710\n",
            " - 15s - loss: 0.4525 - acc: 0.7920 - val_loss: 0.4665 - val_acc: 0.7710\n",
            "Epoch 45/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.5039 - acc: 0.7480\n",
            " - 15s - loss: 0.4538 - acc: 0.7825 - val_loss: 0.5039 - val_acc: 0.7480\n",
            "Epoch 46/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4512 - acc: 0.7940\n",
            " - 15s - loss: 0.4558 - acc: 0.7885 - val_loss: 0.4512 - val_acc: 0.7940\n",
            "Epoch 47/100\n",
            "50/50 [==============================] - 4s 74ms/step - loss: 0.4508 - acc: 0.7850\n",
            " - 15s - loss: 0.4509 - acc: 0.7840 - val_loss: 0.4508 - val_acc: 0.7850\n",
            "Epoch 48/100\n",
            "50/50 [==============================] - 4s 86ms/step - loss: 0.4572 - acc: 0.7730\n",
            " - 18s - loss: 0.4430 - acc: 0.7860 - val_loss: 0.4572 - val_acc: 0.7730\n",
            "Epoch 49/100\n",
            "50/50 [==============================] - 4s 70ms/step - loss: 0.4560 - acc: 0.7880\n",
            " - 16s - loss: 0.4326 - acc: 0.7940 - val_loss: 0.4560 - val_acc: 0.7880\n",
            "Epoch 50/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4495 - acc: 0.7710\n",
            " - 15s - loss: 0.4355 - acc: 0.7970 - val_loss: 0.4495 - val_acc: 0.7710\n",
            "Epoch 51/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4724 - acc: 0.7740\n",
            " - 15s - loss: 0.4343 - acc: 0.7960 - val_loss: 0.4724 - val_acc: 0.7740\n",
            "Epoch 52/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4549 - acc: 0.7880\n",
            " - 15s - loss: 0.4412 - acc: 0.7925 - val_loss: 0.4549 - val_acc: 0.7880\n",
            "Epoch 53/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4296 - acc: 0.8000\n",
            " - 17s - loss: 0.4444 - acc: 0.7995 - val_loss: 0.4296 - val_acc: 0.8000\n",
            "Epoch 54/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4435 - acc: 0.7900\n",
            " - 15s - loss: 0.4372 - acc: 0.7965 - val_loss: 0.4435 - val_acc: 0.7900\n",
            "Epoch 55/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4234 - acc: 0.7990\n",
            " - 15s - loss: 0.4352 - acc: 0.7955 - val_loss: 0.4234 - val_acc: 0.7990\n",
            "Epoch 56/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4370 - acc: 0.8020\n",
            " - 15s - loss: 0.4355 - acc: 0.7990 - val_loss: 0.4370 - val_acc: 0.8020\n",
            "Epoch 57/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4762 - acc: 0.7880\n",
            " - 15s - loss: 0.4164 - acc: 0.8120 - val_loss: 0.4762 - val_acc: 0.7880\n",
            "Epoch 58/100\n",
            "50/50 [==============================] - 4s 71ms/step - loss: 0.4433 - acc: 0.8000\n",
            " - 17s - loss: 0.4196 - acc: 0.8045 - val_loss: 0.4433 - val_acc: 0.8000\n",
            "Epoch 59/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4109 - acc: 0.8130\n",
            " - 16s - loss: 0.4113 - acc: 0.8140 - val_loss: 0.4109 - val_acc: 0.8130\n",
            "Epoch 60/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4913 - acc: 0.7690\n",
            " - 15s - loss: 0.4243 - acc: 0.8035 - val_loss: 0.4913 - val_acc: 0.7690\n",
            "Epoch 61/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4293 - acc: 0.8050\n",
            " - 15s - loss: 0.3985 - acc: 0.8140 - val_loss: 0.4293 - val_acc: 0.8050\n",
            "Epoch 62/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4289 - acc: 0.8000\n",
            " - 15s - loss: 0.4062 - acc: 0.8220 - val_loss: 0.4289 - val_acc: 0.8000\n",
            "Epoch 63/100\n",
            "50/50 [==============================] - 4s 81ms/step - loss: 0.4450 - acc: 0.7860\n",
            " - 17s - loss: 0.4208 - acc: 0.8005 - val_loss: 0.4450 - val_acc: 0.7860\n",
            "Epoch 64/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4086 - acc: 0.8100\n",
            " - 15s - loss: 0.4094 - acc: 0.8080 - val_loss: 0.4086 - val_acc: 0.8100\n",
            "Epoch 65/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.5059 - acc: 0.7540\n",
            " - 15s - loss: 0.4137 - acc: 0.8185 - val_loss: 0.5059 - val_acc: 0.7540\n",
            "Epoch 66/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4531 - acc: 0.7950\n",
            " - 15s - loss: 0.4057 - acc: 0.8205 - val_loss: 0.4531 - val_acc: 0.7950\n",
            "Epoch 67/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4449 - acc: 0.7870\n",
            " - 15s - loss: 0.4043 - acc: 0.8220 - val_loss: 0.4449 - val_acc: 0.7870\n",
            "Epoch 68/100\n",
            "50/50 [==============================] - 4s 85ms/step - loss: 0.4296 - acc: 0.7940\n",
            " - 16s - loss: 0.4016 - acc: 0.8180 - val_loss: 0.4296 - val_acc: 0.7940\n",
            "Epoch 69/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4093 - acc: 0.8260\n",
            " - 16s - loss: 0.3991 - acc: 0.8080 - val_loss: 0.4093 - val_acc: 0.8260\n",
            "Epoch 70/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4092 - acc: 0.8110\n",
            " - 15s - loss: 0.4010 - acc: 0.8255 - val_loss: 0.4092 - val_acc: 0.8110\n",
            "Epoch 71/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4038 - acc: 0.8110\n",
            " - 15s - loss: 0.3935 - acc: 0.8215 - val_loss: 0.4038 - val_acc: 0.8110\n",
            "Epoch 72/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4150 - acc: 0.8270\n",
            " - 15s - loss: 0.3831 - acc: 0.8210 - val_loss: 0.4150 - val_acc: 0.8270\n",
            "Epoch 73/100\n",
            "50/50 [==============================] - 4s 81ms/step - loss: 0.4055 - acc: 0.8150\n",
            " - 16s - loss: 0.3985 - acc: 0.8180 - val_loss: 0.4055 - val_acc: 0.8150\n",
            "Epoch 74/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4529 - acc: 0.8010\n",
            " - 16s - loss: 0.3786 - acc: 0.8270 - val_loss: 0.4529 - val_acc: 0.8010\n",
            "Epoch 75/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4396 - acc: 0.7960\n",
            " - 15s - loss: 0.3896 - acc: 0.8280 - val_loss: 0.4396 - val_acc: 0.7960\n",
            "Epoch 76/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4058 - acc: 0.8150\n",
            " - 15s - loss: 0.3998 - acc: 0.8185 - val_loss: 0.4058 - val_acc: 0.8150\n",
            "Epoch 77/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4143 - acc: 0.8080\n",
            " - 15s - loss: 0.3786 - acc: 0.8280 - val_loss: 0.4143 - val_acc: 0.8080\n",
            "Epoch 78/100\n",
            "50/50 [==============================] - 4s 75ms/step - loss: 0.3939 - acc: 0.8260\n",
            " - 15s - loss: 0.3866 - acc: 0.8230 - val_loss: 0.3939 - val_acc: 0.8260\n",
            "Epoch 79/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4857 - acc: 0.7820\n",
            " - 17s - loss: 0.3848 - acc: 0.8285 - val_loss: 0.4857 - val_acc: 0.7820\n",
            "Epoch 80/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4199 - acc: 0.8060\n",
            " - 15s - loss: 0.3664 - acc: 0.8400 - val_loss: 0.4199 - val_acc: 0.8060\n",
            "Epoch 81/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4169 - acc: 0.8110\n",
            " - 15s - loss: 0.3767 - acc: 0.8375 - val_loss: 0.4169 - val_acc: 0.8110\n",
            "Epoch 82/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4239 - acc: 0.7980\n",
            " - 15s - loss: 0.3934 - acc: 0.8150 - val_loss: 0.4239 - val_acc: 0.7980\n",
            "Epoch 83/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4132 - acc: 0.8180\n",
            " - 15s - loss: 0.3657 - acc: 0.8400 - val_loss: 0.4132 - val_acc: 0.8180\n",
            "Epoch 84/100\n",
            "50/50 [==============================] - 4s 74ms/step - loss: 0.4062 - acc: 0.8200\n",
            " - 17s - loss: 0.3700 - acc: 0.8315 - val_loss: 0.4062 - val_acc: 0.8200\n",
            "Epoch 85/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.3948 - acc: 0.8160\n",
            " - 15s - loss: 0.3720 - acc: 0.8325 - val_loss: 0.3948 - val_acc: 0.8160\n",
            "Epoch 86/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4519 - acc: 0.7900\n",
            " - 15s - loss: 0.3697 - acc: 0.8355 - val_loss: 0.4519 - val_acc: 0.7900\n",
            "Epoch 87/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4025 - acc: 0.8280\n",
            " - 15s - loss: 0.3720 - acc: 0.8295 - val_loss: 0.4025 - val_acc: 0.8280\n",
            "Epoch 88/100\n",
            "50/50 [==============================] - 4s 76ms/step - loss: 0.4224 - acc: 0.8120\n",
            " - 15s - loss: 0.3457 - acc: 0.8490 - val_loss: 0.4224 - val_acc: 0.8120\n",
            "Epoch 89/100\n",
            "50/50 [==============================] - 4s 80ms/step - loss: 0.4540 - acc: 0.8010\n",
            " - 18s - loss: 0.3501 - acc: 0.8460 - val_loss: 0.4540 - val_acc: 0.8010\n",
            "Epoch 90/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4356 - acc: 0.8060\n",
            " - 15s - loss: 0.3739 - acc: 0.8320 - val_loss: 0.4356 - val_acc: 0.8060\n",
            "Epoch 91/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.3992 - acc: 0.8240\n",
            " - 15s - loss: 0.3679 - acc: 0.8305 - val_loss: 0.3992 - val_acc: 0.8240\n",
            "Epoch 92/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.4135 - acc: 0.8220\n",
            " - 15s - loss: 0.3594 - acc: 0.8395 - val_loss: 0.4135 - val_acc: 0.8220\n",
            "Epoch 93/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.3844 - acc: 0.8330\n",
            " - 15s - loss: 0.3516 - acc: 0.8440 - val_loss: 0.3844 - val_acc: 0.8330\n",
            "Epoch 94/100\n",
            "50/50 [==============================] - 4s 84ms/step - loss: 0.3982 - acc: 0.8310\n",
            " - 16s - loss: 0.3419 - acc: 0.8430 - val_loss: 0.3982 - val_acc: 0.8310\n",
            "Epoch 95/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.6083 - acc: 0.7340\n",
            " - 16s - loss: 0.3522 - acc: 0.8435 - val_loss: 0.6083 - val_acc: 0.7340\n",
            "Epoch 96/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.5600 - acc: 0.7660\n",
            " - 15s - loss: 0.3650 - acc: 0.8395 - val_loss: 0.5600 - val_acc: 0.7660\n",
            "Epoch 97/100\n",
            "50/50 [==============================] - 3s 69ms/step - loss: 0.4427 - acc: 0.8010\n",
            " - 15s - loss: 0.3401 - acc: 0.8465 - val_loss: 0.4427 - val_acc: 0.8010\n",
            "Epoch 98/100\n",
            "50/50 [==============================] - 4s 79ms/step - loss: 0.4131 - acc: 0.8210\n",
            " - 16s - loss: 0.3413 - acc: 0.8485 - val_loss: 0.4131 - val_acc: 0.8210\n",
            "Epoch 99/100\n",
            "50/50 [==============================] - 4s 82ms/step - loss: 0.3797 - acc: 0.8300\n",
            " - 16s - loss: 0.3329 - acc: 0.8525 - val_loss: 0.3797 - val_acc: 0.8300\n",
            "Epoch 100/100\n",
            "50/50 [==============================] - 3s 70ms/step - loss: 0.3836 - acc: 0.8540\n",
            " - 16s - loss: 0.3433 - acc: 0.8475 - val_loss: 0.3836 - val_acc: 0.8540\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dzA4xaHgGAIs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oEipcbuzKjAf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training with Augmentation and Dropout"
      ]
    },
    {
      "metadata": {
        "id": "9YbdVavNGALH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-4),\n",
        "              metrics=['acc'])\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CNntsZviGAN8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}